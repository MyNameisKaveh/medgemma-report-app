import gradio as gr
from PIL import Image # ุจุฑุง ฺฉุงุฑ ุจุง ุชุตุงูุฑ ูุงุฒู ุงุณุชุ ุญุช ุงฺฏุฑ ูุฏู ุจุงุฑฺฏุฐุงุฑ ูุดูุฏ
# import requests # ูุนูุง ูุงุฒ ูุณุช
import torch # ููฺูุงู ุจุฑุง ุจุฎุดโูุง ุงุฒ transformers ูุงุฒู ุงุณุช
import os

# --- ุชูฺฉู Hugging Face ุจุฑุง ุฏุณุชุฑุณ ุจู ูุฏูโูุง Gated ุชูุณุท app.py ---
# ุงู ฺฉุฏุ ููุช ุฏุฑ Hugging Face Space ุดูุง ุงุฌุฑุง ูโุดูุฏุ ุงูุชุธุงุฑ ุฏุงุฑุฏ HF_TOKEN
# ุจู ุนููุงู ฺฉ "Secret" ุฏุฑ ุชูุธูุงุช Space ุดูุง ุชูุธู ุดุฏู ุจุงุดุฏ.
HUGGING_FACE_TOKEN = os.getenv("HF_TOKEN") # ุงู ูุชุบุฑ ุฎูุงูุฏู ูโุดูุฏ ุงูุง ูุนูุงู ุงุณุชูุงุฏู ููโุดูุฏ ฺูู ูุฏู ุจุงุฑฺฏุฐุงุฑ ููโุดูุฏ

# --- ูุชุบุฑูุง ุณุฑุงุณุฑ ุจุฑุง ูุฏู ู ูุถุนุช ุจุงุฑฺฏุฐุงุฑ ---
model_pipeline = None
model_load_error_message = "Model loading is currently disabled to prevent memory issues on the current hardware. Please switch to an API-based solution or upgrade hardware."
model_loaded_successfully = False # ููู: ุงู ุฑุง False ูฺฏู ูโุฏุงุฑู

# --- ููุทู ุจุงุฑฺฏุฐุงุฑ ูุฏู (ูููุชุงู ุจู ุทูุฑ ฺฉุงูู ุบุฑูุนุงู ุดุฏู) ---
print("MedGemma App: Starting up...")
print(f"MedGemma App: Model loading for 'google/medgemma-4b-it' is TEMPORARILY DISABLED in this version of app.py.")
print(f"MedGemma App: Reason: To prevent Out Of Memory errors on CPU basic tier and to prepare for API integration.")
print(f"MedGemma App: The Gradio interface will load, but report generation will indicate model is not available.")
#
# # # ฺฉุฏ ุงุตู ุจุงุฑฺฏุฐุงุฑ ูุฏู (ฺฉุงููุงู ฺฉุงููุช ุดุฏู ุงุณุช ุชุง ุงุฌุฑุง ูุดูุฏ)
# # try:
# #     print("MedGemma App: Attempting to load google/medgemma-4b-it model...")
# #     device_to_use = "cpu"
# #     dtype_to_use = torch.float32
# #     print(f"MedGemma App: Using device: {device_to_use}.")
# #
# #     # from transformers import pipeline # ุงู import ูู ุงฺฏุฑ ููุท ุงู ุจุฎุด ฺฉุงููุช ุดูุฏุ ุจุงุฏ ุจุฑุฑุณ ุดูุฏ
# #     model_pipeline = pipeline(
# #         "image-text-to-text",
# #         model="google/medgemma-4b-it",
# #         torch_dtype=dtype_to_use,
# #         device=device_to_use,
# #         token=HUGGING_FACE_TOKEN
# #     )
# #     model_loaded_successfully = True
# #     print("MedGemma App: Model loaded successfully.")
# # except Exception as e:
# #     model_load_error_message = str(e)
# #     print(f"MedGemma App: CRITICAL ERROR loading model: {model_load_error_message}")
# #     # model_pipeline = None # ุงุทููุงู ุงุฒ ุงูฺฉู ุฏุฑ ุตูุฑุช ุฎุทุง None ุงุณุช
# #     # model_loaded_successfully = False # ุงุทููุงู ุงุฒ ุงูฺฉู ุฏุฑ ุตูุฑุช ุฎุทุง False ุงุณุช
#

# --- ุชุงุจุน ุงุตู ุจุฑุง ูพุฑุฏุงุฒุด ุจุง Gradio ---
def generate_medical_report(input_image_pil, user_prompt_text):
    if not model_loaded_successfully: # ุงู ุดุฑุท ููุดู True ุฎูุงูุฏ ุจูุฏ ฺูู ูุฏู ุจุงุฑฺฏุฐุงุฑ ูุดุฏู
        print(f"MedGemma App: generate_medical_report called, but model is not loaded. Error message: {model_load_error_message}")
        return f"ุฎุทุง: ูุฏู ุจุงุฑฺฏุฐุงุฑ ูุดุฏู ุงุณุช. {model_load_error_message}"
    
    # ุงู ุจุฎุด ุงุฒ ฺฉุฏ ูุฑฺฏุฒ ุงุฌุฑุง ูุฎูุงูุฏ ุดุฏ ุชุง ุฒูุงู ฺฉู model_loaded_successfully ุจู True ุชุบุฑ ฺฉูุฏ
    if input_image_pil is None:
        return "ุฎุทุง: ูุทูุงู ฺฉ ุชุตูุฑ ุขูพููุฏ ฺฉูุฏ."
    if not user_prompt_text or not user_prompt_text.strip():
        return "ุฎุทุง: ูุทูุงู ฺฉ ุณูุงู ุง ุชูุถุญ ูุงุฑุฏ ฺฉูุฏ."

    print(f"MedGemma App: Processing request (this part should not be reached with current settings). Prompt: '{user_prompt_text}'.")
    messages = [
        # ... (ุณุงุฎุชุงุฑ ูพุงูโูุง ูุซู ูุจู) ...
    ]
    try:
        # raw_output = model_pipeline(text=messages, max_new_tokens=512)
        # ... (ุจูู ฺฉุฏ ูพุฑุฏุงุฒุด ุฎุฑูุฌ) ...
        # final_text = "..."
        # return final_text
        return "ุงู ุจุฎุด ูุจุงุฏ ุงุฌุฑุง ุดูุฏ ุฒุฑุง ูุฏู ุจุงุฑฺฏุฐุงุฑ ูุดุฏู ุงุณุช." # ูพุงู ุฌุงฺฏุฒู
    except Exception as e:
        # ... (ูุฏุฑุช ุฎุทุง) ...
        return f"ุฎุทุง ุฏุฑ ููฺฏุงู ุชููุฏ ฺฏุฒุงุฑุด (ุงู ุจุฎุด ูุจุงุฏ ุงุฌุฑุง ุดูุฏ): {str(e)}"

# --- ุชุนุฑู ุฑุงุจุท ฺฉุงุฑุจุฑ Gradio ---
disclaimer_markdown = """
---
**โ๏ธ ุณูุจ ูุณุฆููุช ููู (Disclaimer):**

ุงู ุงูพูฺฉุดู ฺฉ ููููู ููุงุด ุงุณุช. ูุฏู ููุด ูุตููุน MedGemma (`google/medgemma-4b-it`) ุฏุฑ ุงู ูุณุฎู ุจุฑุง ุฌููฺฏุฑ ุงุฒ ูุดฺฉูุงุช ุญุงูุธู ุฏุฑ ุณุฎุชโุงูุฒุงุฑ ูุนู **ุจุงุฑฺฏุฐุงุฑ ูุดุฏู ุงุณุช.**
ุงุทูุงุนุงุช ู ฺฏุฒุงุฑุดโูุง ุชููุฏ ุดุฏู ุชูุณุท ูุฏู ุงุตู (ุฏุฑ ุตูุฑุช ูุนุงู ุจูุฏู) **ุจู ูฺ ุนููุงู ูุจุงุฏ ุจู ุนููุงู ูุดุงูุฑูุ ุชุดุฎุตุ ุง ุชูุตู ุฏุฑูุงู ูพุฒุดฺฉ ุญุฑููโุง ุชูู ุดููุฏ.**

* ููุดู ุจุฑุง ูุฑฺฏููู ุณูุงู ุง ูฺฏุฑุงู ุฏุฑ ููุฑุฏ ูุถุนุช ูพุฒุดฺฉ ุฎูุฏ ุง ุฏฺฏุฑุงูุ ุจุง ูพุฒุดฺฉ ุง ูุชุฎุตุต ูุงุฌุฏ ุดุฑุงุท ูุดูุฑุช ฺฉูุฏ.
* ุงุณุชูุงุฏู ุงุฒ ูุฏู MedGemma ุชุญุช ุดุฑุงุท ู ููุงูู "Health AI Developer Foundations" ฺฏูฺฏู ุงุณุช.

**ุงู ุงุจุฒุงุฑ ุฌุงฺฏุฒู ูุถุงูุช ุจุงูู ฺฉ ูุชุฎุตุต ูพุฒุดฺฉ ูุณุช.**
"""

with gr.Blocks(theme=gr.themes.Soft(primary_hue=gr.themes.colors.blue, secondary_hue=gr.themes.colors.sky)) as demo_interface:
    gr.Markdown("# ๐ฉบ MedGemma - ุฏุณุชุงุฑ ฺฏุฒุงุฑุดโุฏู ูพุฒุดฺฉ (ูุณุฎู ุขุฒูุงุด - ูุฏู ุบุฑูุนุงู)")
    gr.Markdown(disclaimer_markdown)

    # ููุงุด ูุถุนุช ุจุงุฑฺฏุฐุงุฑ ูุฏู
    if not model_loaded_successfully:
        gr.Warning(f"ุชูุฌู: ุจุงุฑฺฏุฐุงุฑ ูุฏู ุงุตู MedGemma ุฏุฑ ุงู ูุณุฎู ุบุฑูุนุงู ุดุฏู ุงุณุช. {model_load_error_message}")
    else: # ุงู ุญุงูุช ูุนูุงู ุฑุฎ ููโุฏูุฏ
        gr.Success("ูุฏู ุจุง ููููุช ุจุงุฑฺฏุฐุงุฑ ุดุฏ ู ุขูุงุฏู ุงุณุชูุงุฏู ุงุณุช.")

    with gr.Row():
        with gr.Column(scale=1):
            input_image_component = gr.Image(type="pil", label="ฑ. ุชุตูุฑ ูพุฒุดฺฉ ุฑุง ุขูพููุฏ ฺฉูุฏ")
            input_prompt_component = gr.Textbox(lines=3, label="ฒ. ุณูุงู ุง ุฏุฑุฎูุงุณุช ุฎูุฏ ุฑุง ุจููุณุฏ:", placeholder="ูุซุงู: ุงู ุชุตูุฑ ุฑุง ุชูุตู ฺฉู.")
            submit_button_component = gr.Button("๐ ุชููุฏ ฺฏุฒุงุฑุด (ูุฏู ุบุฑูุนุงู)", variant="primary")
        with gr.Column(scale=1):
            output_report_component = gr.Textbox(lines=15, label="ูุชุฌู ุชุญูู ูุฏู:", interactive=False, show_copy_button=True)
            
    submit_button_component.click(
        fn=generate_medical_report,
        inputs=[input_image_component, input_prompt_component],
        outputs=output_report_component
    )

# --- ุฑุงูโุงูุฏุงุฒ ุงูพูฺฉุดู Gradio ---
if __name__ == "__main__":
    demo_interface.launch()
